{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1yvuMfTMiP7SJC9m6azou37p4rn-TbRfp",
      "authorship_tag": "ABX9TyNaLX43/Ig4B/RU4K1DKmN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yermakhan-s/Amazon-Product-Review-Sentiment-Analysis-BERT-model/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE_D_2Ybt_0G"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, BertModel, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFyDb3ldunGZ",
        "outputId": "0a6eb4e5-3fc8-4ef5-dba5-c28a88fd3a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBVZSKliuHgm",
        "outputId": "27dcc9ae-a938-4665-f18e-f6981f18a92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = []\n",
        "texts = []\n",
        "\n",
        "for d in dataset['full']:\n",
        "    ratings.append(d['rating'])\n",
        "    texts.append(d['text'])"
      ],
      "metadata": {
        "id": "IBWNxLCtuJUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(ratings)):\n",
        "    if ratings[i] == 1 or ratings[i] == 2:\n",
        "        ratings[i] = 1\n",
        "    elif ratings[i] == 3:\n",
        "        ratings[i] = 2\n",
        "    elif ratings[i] == 4 or ratings[i] == 5:\n",
        "        ratings[i] = 3"
      ],
      "metadata": {
        "id": "shfbkmMouMeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = {1: 0, 2: 0, 3: 0}\n",
        "\n",
        "# Count occurrences of each value\n",
        "for value in ratings:\n",
        "    counts[value] += 1\n",
        "\n",
        "print(\"Number of 0s:\", counts[1])\n",
        "print(\"Number of 1s:\", counts[2])\n",
        "print(\"Number of 2s:\", counts[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp57Z-YkuzGg",
        "outputId": "841ff4ca-d3a3-45da-a07a-72ada0fe4e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 0s: 145114\n",
            "Number of 1s: 56307\n",
            "Number of 2s: 500107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values = set(ratings)\n",
        "\n",
        "# Convert the set back to a list if needed\n",
        "unique_list = list(unique_values)\n",
        "\n",
        "print(unique_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "79CM9Adau2j7",
        "outputId": "b42198a0-4d36-4406-90a2-b4c86be3d6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ratings' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5851044fd9e9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Convert the set back to a list if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munique_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ratings' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "class_counts = {1: 0, 2: 0, 3: 0}\n",
        "\n",
        "new_ratings = []\n",
        "new_texts = []\n",
        "# Iterate over ratings and texts simultaneously\n",
        "for rating, text in zip(ratings, texts):\n",
        "    if class_counts[rating] < 10000:\n",
        "        new_ratings.append(rating)\n",
        "        new_texts.append(text)\n",
        "        class_counts[rating] += 1\n",
        "    # If we have collected 25000 examples from each class, stop\n",
        "    if all(count == 10000 for count in class_counts.values()):\n",
        "        break\n"
      ],
      "metadata": {
        "id": "vpS5zWGwu55v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'text': new_texts, 'rating': new_ratings}\n",
        "df = pd.DataFrame(data)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vlXdIGZMu7zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x1OyaJP3u9pZ",
        "outputId": "823208fc-a15c-4882-f073-3f78e210a4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = len(df['rating'].unique()), # number of unique labels for our multi-class classification problem\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHYJchoxvGow",
        "outputId": "1879e338-6c8b-4e29-82e1-d191b860b6d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "    def __init__(self, df, max_length=512):\n",
        "        self.df = df\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # input=review, label=stars\n",
        "        review = self.df.loc[idx, 'text']\n",
        "        # labels are 0-indexed\n",
        "        label = int(self.df.loc[idx, 'rating']) - 1\n",
        "\n",
        "        encoded = self.tokenizer(\n",
        "            review,                      # review to encode\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,  # Truncate all segments to max_length\n",
        "            padding='max_length',        # pad all reviews with the [PAD] token to the max_length\n",
        "            return_attention_mask=True,  # Construct attention masks.\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids = encoded['input_ids']\n",
        "        attn_mask = encoded['attention_mask']\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids),\n",
        "            'attn_mask': torch.tensor(attn_mask),\n",
        "            'label': torch.tensor(label)\n",
        "        }"
      ],
      "metadata": {
        "id": "bIw1vf8EvHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "EprdIK7LvI_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "TEST_SIZE = 0.2\n",
        "VAL_SIZE = 0.125\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 32\n",
        "\n",
        "CHECKPOINT_FILE = 'checkpoint.dat'\n",
        "CHECKPOINT_FOLDER = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 2e-05\n",
        "PROJECT_FOLDER = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "MODEL_FOLDER = 'Model'\n",
        "SAVE_EVERY = 100\n",
        "NUM_WORKERS = 4"
      ],
      "metadata": {
        "id": "q3b1wPnxvKdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = train_test_split(df, test_size=TEST_SIZE, random_state=1)\n",
        "train_dataset, val_dataset = train_test_split(train_dataset, test_size=VAL_SIZE, random_state=1)\n",
        "\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "val_dataset = val_dataset.reset_index(drop=True)\n",
        "test_dataset = test_dataset.reset_index(drop=True)\n",
        "\n",
        "train_set = ReviewsDataset(train_dataset, MAX_LEN)\n",
        "val_set = ReviewsDataset(val_dataset, MAX_LEN)\n",
        "test_set = ReviewsDataset(test_dataset, MAX_LEN)\n",
        "\n",
        "print(\"# of samples in train set: {}\".format(len(train_set)))\n",
        "print(\"# of samples in val set: {}\".format(len(val_set)))\n",
        "print(\"# of samples in test set: {}\".format(len(test_set)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUwEy6XhvMD_",
        "outputId": "265fe12e-2ebe-48ca-fe5b-b81d9b326b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of samples in train set: 21000\n",
            "# of samples in val set: 3000\n",
            "# of samples in test set: 6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {\n",
        "                'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': NUM_WORKERS\n",
        "                }\n",
        "val_params = train_params\n",
        "\n",
        "test_params = {\n",
        "                'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': NUM_WORKERS\n",
        "              }\n",
        "\n",
        "train_loader = DataLoader(train_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)\n",
        "test_loader = DataLoader(test_set, **test_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y_rKefPvN6Q",
        "outputId": "71efb579-f920-4d84-efdc-1178ade0f7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For weighted Cross Entropy Loss\n",
        "# Penalize errors higher if they come from a class with lower frequency\n",
        "star_groups = df.groupby('rating')\n",
        "print(star_groups.head())\n",
        "star_distribution = []\n",
        "for i in range(len(df['rating'].unique())):\n",
        "    star_distribution.append(len(star_groups.groups[i+1])/len(df))\n",
        "\n",
        "star_distribution = torch.tensor(star_distribution, dtype=torch.float32)\n",
        "\n",
        "# V3\n",
        "weights = 1.0 / star_distribution\n",
        "weights = weights / weights.sum()\n",
        "\n",
        "# V4\n",
        "# weights = 1.0 - star_distribution\n",
        "\n",
        "print('{:<20}: {}'.format('Star distribution', star_distribution.tolist()))\n",
        "print('{:<20}: {}'.format('Weights', weights.tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DBOVbIdvQ-I",
        "outputId": "aedf6200-5134-44b8-a113-f72453dae0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 text  rating\n",
            "0   This spray is really nice. It smells really go...       3\n",
            "1   This product does what I need it to do, I just...       3\n",
            "2                           Smells good, feels great!       3\n",
            "3                                      Felt synthetic       1\n",
            "4                                             Love it       3\n",
            "5   The polish was quiet thick and did not apply s...       3\n",
            "7   These were lightweight and soft but much too s...       2\n",
            "10  I was very disappointed when I got this facial...       2\n",
            "12  I try to get Keratin treatments every 3 months...       2\n",
            "15  When I saw this, I was thrilled to be able to ...       2\n",
            "22  I don't see the fuss with this toothbrush. As ...       2\n",
            "27  nothing special  unfortunately i waited too lo...       1\n",
            "29  I think I need to stick with my 5 blade razor....       1\n",
            "30  A total waste of money.  I get better results ...       1\n",
            "32  This halo hair extension is simply put, garbag...       1\n",
            "Star distribution   : [0.3333333432674408, 0.3333333432674408, 0.3333333432674408]\n",
            "Weights             : [0.3333333432674408, 0.3333333432674408, 0.3333333432674408]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss(weight=weights.to(device), reduction='mean')\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "A9EAYxk_vTVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the accuracy function\n",
        "def calculate_accuracy(big_idx, targets):\n",
        "    n_correct = (big_idx==targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "J0XUGSbVvbVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For validation\n",
        "def validate(model, data_loader):\n",
        "    model.eval()\n",
        "    n_correct = 0\n",
        "    nb_test_steps = 0\n",
        "    nb_test_examples = 0\n",
        "    test_loss = 0\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(data_loader, 0):\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            mask = data['attn_mask'].to(device)\n",
        "            labels = data['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, mask)\n",
        "            loss = loss_function(outputs[0], labels)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # gets labels with highest probabilities and their corresponding indices\n",
        "            big_val, big_idx = torch.max(outputs[0].data, dim=1)\n",
        "            n_correct += calculate_accuracy(big_idx, labels)\n",
        "\n",
        "            preds = (big_idx + 1).cpu().tolist()\n",
        "            gold = (labels + 1).cpu().tolist()\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(gold)\n",
        "\n",
        "            nb_test_steps += 1\n",
        "            nb_test_examples += labels.size(0)\n",
        "\n",
        "    epoch_loss = test_loss/nb_test_steps\n",
        "    epoch_accu = (n_correct*100)/nb_test_examples\n",
        "    print(f\"Validation Loss: {epoch_loss}\")\n",
        "    print(f\"Validation Accuracy: {epoch_accu}\\n\")\n",
        "\n",
        "    return y_true, y_pred, epoch_accu"
      ],
      "metadata": {
        "id": "jpBg_LfQveBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(epoch):\n",
        "    # number of batches run by model\n",
        "    nb_tr_steps = 0\n",
        "    # number of training examples run by model\n",
        "    nb_tr_examples = 0\n",
        "    # number of examples classified correctly by model\n",
        "    n_correct = 0\n",
        "    tr_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch, data in enumerate(train_loader):\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        mask = data['attn_mask'].to(device)\n",
        "        labels = data['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, mask)\n",
        "        loss = loss_function(outputs[0], labels)\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        # gets labels with highest probabilities and their corresponding indices\n",
        "        big_val, big_idx = torch.max(outputs[0].data, dim=1)\n",
        "        n_correct += calculate_accuracy(big_idx, labels)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=labels.size(0)\n",
        "\n",
        "        if batch % SAVE_EVERY == 0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples\n",
        "            print(\"Batch {} of epoch {} complete.\".format(batch, epoch+1))\n",
        "            print(f\"Training Loss: {loss_step}   Training Accuracy: {accu_step}\")\n",
        "\n",
        "            if not os.path.exists(CHECKPOINT_FOLDER):\n",
        "              os.makedirs(CHECKPOINT_FOLDER)\n",
        "\n",
        "            # Since a single epoch could take well over hours, we regularly save the model even during evaluation of training accuracy.\n",
        "            torch.save(model.state_dict(), os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))\n",
        "            print(\"Saving checkpoint at\", os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print('\\n*****\\n')\n",
        "    print(f'The Total Accuracy for Epoch {epoch+1}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy: {epoch_accu}\\n\")\n",
        "\n",
        "    # Evaluate model after training it on this epoch\n",
        "    validate(model, val_loader)\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))\n",
        "    model.save_pretrained(os.path.join(PROJECT_FOLDER, MODEL_FOLDER, str(epoch+1)))\n",
        "    print(\"Saving checkpoint at \", os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))\n",
        "    print(\"Saving model at \", os.path.join(PROJECT_FOLDER, MODEL_FOLDER, str(epoch+1)), '\\n\\n================================================\\n')\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "9WuhuPy9vgSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training without weighted loss\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kklr9zZ4vi2Y",
        "outputId": "4257a058-ec78-44c2-b037-48052bcb67d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0 of epoch 1 complete.\n",
            "Training Loss: 1.1147043704986572   Training Accuracy: 34.375\n",
            "Saving checkpoint at /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n",
            "Batch 100 of epoch 1 complete.\n",
            "Training Loss: 0.8339582268554385   Training Accuracy: 60.51980198019802\n",
            "Saving checkpoint at /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n",
            "Batch 200 of epoch 1 complete.\n",
            "Training Loss: 0.7307419619750027   Training Accuracy: 66.83768656716418\n",
            "Saving checkpoint at /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n",
            "Batch 300 of epoch 1 complete.\n",
            "Training Loss: 0.7000133623910505   Training Accuracy: 68.75\n",
            "Saving checkpoint at /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n",
            "Batch 400 of epoch 1 complete.\n",
            "Training Loss: 0.6706147437678311   Training Accuracy: 70.19170822942644\n",
            "Saving checkpoint at /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n",
            "Batch 500 of epoch 1 complete.\n",
            "Training Loss: 0.6517189834527151   Training Accuracy: 71.19510978043913\n",
            "Saving checkpoint at /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n",
            "Batch 600 of epoch 1 complete.\n",
            "Training Loss: 0.6376672569606546   Training Accuracy: 71.88019966722129\n",
            "Saving checkpoint at /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*****\n",
            "\n",
            "The Total Accuracy for Epoch 1: 72.19047619047619\n",
            "Training Loss: 0.6316544397690161\n",
            "Training Accuracy: 72.19047619047619\n",
            "\n",
            "Validation Loss: 0.5673280091995888\n",
            "Validation Accuracy: 75.46666666666667\n",
            "\n",
            "Saving checkpoint at  /content/drive/MyDrive/Colab Notebooks/checkpoint.dat\n",
            "Saving model at  /Model_V3/1 \n",
            "\n",
            "================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation on test set\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "  model = BertForSequenceClassification.from_pretrained(os.path.join(PROJECT_FOLDER, MODEL_FOLDER, str(epoch))).cuda()\n",
        "  print(f'Running validation on model trained on {epoch} epochs')\n",
        "\n",
        "  validate(model, test_loader)"
      ],
      "metadata": {
        "id": "V-NLrfUc0JbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(os.path.join(PROJECT_FOLDER, MODEL_FOLDER, '1')).cuda()\n",
        "print(f'Running validation on model trained on 1 epochs')\n",
        "\n",
        "y_true, y_pred, epoch_acc = validate(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "pr_FVYG1vlqJ",
        "outputId": "2c60e2a3-f8d1-4774-bc9f-cedc355e84c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'BertForSequenceClassification' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c557fd80ea35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMODEL_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running validation on model trained on 1 epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_single_prediction(comment, model):\n",
        "  \"\"\"\n",
        "  Predict a star rating from a review comment.\n",
        "\n",
        "  :comment: the string containing the review comment.\n",
        "  :model: the model to be used for the prediction.\n",
        "  \"\"\"\n",
        "\n",
        "  df = pd.DataFrame()\n",
        "  df['text'] = [comment]\n",
        "  df['rating'] = ['0']\n",
        "\n",
        "  dataset = ReviewsDataset(df)\n",
        "\n",
        "  TEST_BATCH_SIZE = 1\n",
        "  NUM_WORKERS = 1\n",
        "\n",
        "  test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "              'shuffle': True,\n",
        "              'num_workers': NUM_WORKERS}\n",
        "\n",
        "  data_loader = DataLoader(dataset, **test_params)\n",
        "\n",
        "  total_examples = len(df)\n",
        "  predictions = np.zeros([total_examples], dtype=object)\n",
        "\n",
        "  for batch, data in enumerate(data_loader):\n",
        "\n",
        "    # Get the tokenization values.\n",
        "    input_ids = data['input_ids'].to(device)\n",
        "    mask = data['attn_mask'].to(device)\n",
        "\n",
        "    # Make the prediction with the trained model.\n",
        "    outputs = model(input_ids, mask)\n",
        "\n",
        "    # Get the star rating.\n",
        "    big_val, big_idx = torch.max(outputs[0].data, dim=1)\n",
        "    star_predictions = (big_idx + 1).cpu().numpy()\n",
        "\n",
        "  return star_predictions[0]\n"
      ],
      "metadata": {
        "id": "SNsHbVxY2asf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the model\n",
        "torch.save(model.state_dict(), os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))\n",
        "model.save_pretrained(os.path.join(PROJECT_FOLDER, MODEL_FOLDER, str(epoch+1)))"
      ],
      "metadata": {
        "id": "4PUj5QdR4VsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(os.path.join(PROJECT_FOLDER, MODEL_FOLDER, str(2))).cuda()"
      ],
      "metadata": {
        "id": "4WG8vHWJ6RLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = get_single_prediction(\"This product is not useful!\", model)"
      ],
      "metadata": {
        "id": "mb9CxPmO5yXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc486b7-2322-4c58-bc2f-a29573c96d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BO_XH5C9oI2",
        "outputId": "188e4da9-536a-4374-8b6d-2d6ecb80a01c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = get_single_prediction(\"This product is alright\", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uCZVBd-49Ub",
        "outputId": "9af8662d-1230-4b4f-9a25-a957b26569ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U93RNuqj5EOP",
        "outputId": "6d31c40c-3b2e-42fa-fb04-b34915e8ec3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = get_single_prediction(\"This product is very good!\", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJWClo2N5NIA",
        "outputId": "3f7db790-3eb2-42da-f8f6-9fa182ab69e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_H11I9w5UQ3",
        "outputId": "8a3c9cba-57ce-46d9-c2cc-c6a4ae1823a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z2RCCGn85WOO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}